# -*- coding: utf-8 -*-
"""Actividad 2. Detección Cráteres.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eYofQ1q7XN2eyzc9g2Rw28yHlyzUoEtd

#**ACTIVIDAD 2. DETECCIÓN DE CRÁTERES EN MARTE Y LA LUNA.**

####En este notebook abordaremos la tarea de entrenar dos modelos de red neuronal de detección de objetos para identificar cráteres en marte y la luna.

##**FASTER R-CNN (Region-based Convolutional Neural Network)**

Faster R-CNN se destaca por su introducción del Region Proposal Network (RPN), una red que propone regiones en la imagen que podrían contener objetos de interés. Esto permite separar la generación de propuestas del proceso de clasificación y regresión, mejorando significativamente la eficiencia y precisión del modelo.

La espina dorsal de Faster R-CNN, que puede ser una red preentrenada como VGG o ResNet, extrae características de la imagen, y luego se utilizan capas de ROI Pooling o ROI Align para ajustar las propuestas a un tamaño fijo antes de la clasificación y regresión.

Crater object detection using Faster RCNN
Table of Contents:

Part 1: Introduction and Setup
Part 2: Dataset class setup
Part 3: Model configuration
Part 4: Image visulisation
Part 5: K-fold cross validation
Part 6: Final training and evaluation
Part 7: Inference
Part 8: Summary

###**1. Introducción y configuración del entorno**

Dercargaremos el repositorio de TorchVision desde GitHub, lo copiaremos en el directorio `vision`, seleccionando la version v0.8.2 del repositorio.

Copiaremos los archivos necesarios desde el repositorio de TorchVision al directorio actual. Estos archivos contienen funciones y utilidades necesarias para el entrenamiento y la evaluación de modelos de detección de objetos (`utils.py, transforms.py, coco_eval.py, engine.py, coco_utils.py`)

Instalamos las dependencias necesarias: paquete de Cython, pycocotools desde GitHub, biblioteca Albumentations (para realizar transformaciones en las imágenes) y biblioteca OpenCV (para manipulación y procesamiento de imágenes).

Finalmente, descargamos y descomprimimos el conjunto de datos que queremos utilizar en este caso desde Google Drive usando `gdown`.
"""

# Download TorchVision repo to use some files from
# references/detection
!git clone https://github.com/pytorch/vision.git
!cd vision
!git checkout v0.8.2

!cp ./vision/references/detection/utils.py ./
!cp ./vision/references/detection/transforms.py ./
!cp ./vision/references/detection/coco_eval.py ./
!cp ./vision/references/detection/engine.py ./
!cp ./vision/references/detection/coco_utils.py ./

!pip install cython
# Install pycocotools, the version by default in Colab
!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'
!pip install -U albumentations
!pip install -U opencv-python

!pip install gdown #Descargamos nuestro conjunto de datos de Google Drive
!gdown https://drive.google.com/uc?id=1hrHgANwgC8VyXLhXgxLYHTrILpeJ5bLl
!unzip /content/mars_and_moon.zip

"""Importamos las bibliotecas y módulos necesarios: PyTorch, TorchVision, OpenCv y otras que se utilizarán a lo largo del entrenamiento y evaluación del modelo."""

import os #Biblioteca con funciones para interactuar con el sistema operativo, como trabajar con rutas de archivos y directorios.
import numpy as np #Biblioteca para cálculos numéricos: análisis de datos y manipulación de matrices.
import torch #PyTorch: construir y entrenar modelos de aprendizaje profundo.
import torchvision #Biblioteca que forma parte de PyTorch y proporciona modelos preentrenados y utilidades para la visión por computadora.
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor #Módulo específico de TorchVision que se utiliza para definir el predictor de la capa de detección de objetos Faster R-CNN.
from engine import train_one_epoch, evaluate
import utils
import transforms as T #Transformaciones de datos utilizadas durante el entrenamiento y la evaluación.
import albumentations as A #Biblioteca que se utiliza para aplicar transformaciones de datos, como aumentaciones, a imágenes y etiquetas de detección de objetos.
import cv2 #OpenCV: biblioteca de visión por computadora que se utiliza para trabajar con imágenes y aplicar diversas operaciones de procesamiento de imágenes.
import time #Esta biblioteca proporciona funciones relacionadas con la medición del tiempo y se utiliza para calcular el tiempo que lleva ejecutar ciertas operaciones.
from albumentations.pytorch.transforms import ToTensorV2 #Importa una transformación de Albumentations que convierte las imágenes en tensores de PyTorch.
import matplotlib #Biblioteca para crear gráficos y visualizaciones
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from  sklearn.model_selection import KFold
import random #Esta biblioteca proporciona funciones relacionadas con la generación de números aleatorios y se utiliza para realizar selecciones aleatorias, como la selección de muestras de datos durante el entrenamiento cruzado (K-fold).

"""###**2. Preparación de los datos. Setup de las clase Dataset**

El siguiente código define la clase Dataset para manejar y hacer las transformaciones necesarias en el conjunto de datos y sus boundary boxes. Además se definen los nombres de las clases.

El constructor define varias funciones:
- `__init__`: recibe dos argumentos `root`(ruta del directorio del conjunto de datos) y `trasnform`. Las imágenes y anotaciones se cargan y almacenan en las listas `self.imgs` y `self.annots`. Se define una lista, `self.classses` con los nombres de las clases: `Background`y `Crater`.
- `convert_box_cord`: convierte las coordenadas de las cajas del formato (x,y,ancho,alto) normalizado, al formato (xmin,ymin,xmax,ymax) requerido por Faster R-CNN.
-`__getitem__`: utilizada para recuperar un elemento específico del conjunto de datos. Lee la imagen y la anotación dle índice proporcionado. Si la anotación no está vacía, las cajas delimitadoras se cargan y convierten usando `convert_box_cord`.Si la anotación está vacía, se crea una caja delimitadora ficticia y se asigna la etiqueta 0 (`background`). Se calcula el área de las cajas delimitadoras y se crea un diccionario target con información sobre las cajas y etiquetas.
- `__len__`devuelve el número total de imágenes en el conjunto de datos.
"""

class CraterDataset(object):
    def __init__(self, root, transforms):
        self.root = root
        self.transforms = transforms
        # load all image files, sorting them to
        # ensure that they are aligned
        self.imgs = list(sorted(os.listdir(os.path.join(self.root, "images"))))
        self.annots = list(sorted(os.listdir(os.path.join(self.root, "labels"))))
        self.classes = ['Background','Crater']

    # Converts boundry box formats, this version assumes single class only!
    def convert_box_cord(self,bboxs, format_from, format_to, img_shape):
        if format_from == 'normxywh':
            if format_to == 'xyminmax':
                xw = bboxs[:, (1, 3)] * img_shape[1]
                yh = bboxs[:, (2, 4)] * img_shape[0]
                xmin = xw[:, 0] - xw[:, 1] / 2
                xmax = xw[:, 0] + xw[:, 1] / 2
                ymin = yh[:, 0] - yh[:, 1] / 2
                ymax = yh[:, 0] + yh[:, 1] / 2
                coords_converted = np.column_stack((xmin, ymin, xmax, ymax))

        return coords_converted

    def __getitem__(self, idx):
        # load images and boxes
        img_path = os.path.join(self.root, "images", self.imgs[idx])
        annot_path = os.path.join(self.root, "labels", self.annots[idx])
        img = cv2.imread(img_path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)
        img= img/255.0

        # retrieve bbox list and format to required type,
        # if annotation file is empty, fill dummy box with label 0
        if os.path.getsize(annot_path) != 0:
            bboxs = np.loadtxt(annot_path, ndmin=2)
            bboxs = self.convert_box_cord(bboxs, 'normxywh', 'xyminmax', img.shape)
            num_objs = len(bboxs)
            bboxs = torch.as_tensor(bboxs, dtype=torch.float32)
            # there is only one class
            labels = torch.ones((num_objs,), dtype=torch.int64)
            # suppose all instances are not crowd
            iscrowd = torch.zeros((num_objs,), dtype=torch.int64)
        else:
            bboxs = torch.as_tensor([[0, 0, 640, 640]], dtype=torch.float32)
            labels = torch.zeros((1,), dtype=torch.int64)
            iscrowd = torch.zeros((1,), dtype=torch.int64)

        area = (bboxs[:, 3] - bboxs[:, 1]) * (bboxs[:, 2] - bboxs[:, 0])
        image_id = torch.tensor([idx])

        target = {}
        target["boxes"] = bboxs
        target["labels"] = labels
        target["image_id"] = image_id
        target["area"] = area
        target["iscrowd"] = iscrowd

        if self.transforms is not None:
            sample = self.transforms(image=img,
                                     bboxes=target['boxes'],
                                     labels=labels)
        img = sample['image']
        target['boxes'] = torch.tensor(sample['bboxes'])
        target['labels'] = torch.tensor(sample['labels'])
        if target['boxes'].ndim == 1:
            target['boxes'] = torch.as_tensor([[0, 0, 640, 640]], dtype=torch.float32)
            target['labels'] = torch.zeros((1,), dtype=torch.int64)
        return img, target

    def __len__(self):
        return len(self.imgs)

"""###**3. Configuración del modelo.**

En este proyecto, se empleó Faster R-CNN con una estructura de resnet50 preentrenada en el conjunto de datos COCO. La cabeza preentrenada de la región de interés (ROI) fue reemplazada por una nueva que aún no ha sido entrenada y que será ajustada a nuestro conjunto de datos.

La función get_transform define las transformaciones que se aplicarán al conjunto de datos antes de presentarlo al modelo.

La función reset_weights restablece todos los pesos entrenables en el modelo. Se realizó un intento breve en este sentido, pero mejorar el modelo preentrenado requeriría un tiempo considerable y un conjunto de datos más extenso.
"""

def get_model_bbox(num_classes):
    # load an instance segmentation model pre-trained on COCO
    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)

    # get number of input features for the classifier
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    # replace the pre-trained head with a new one
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)

    return model

def get_transform(train):
    if train:
        return A.Compose([
            # A.Flip(p=0.5),
            # A.RandomResizedCrop(height=640,width=640,p=0.4),
            # # A.Perspective(p=0.4),
            # A.Rotate(p=0.5),
            # # A.Transpose(p=0.3),
            ToTensorV2(p=1.0)],
            bbox_params=A.BboxParams(format='pascal_voc',min_visibility=0.4, label_fields=['labels']))
    else:
        return A.Compose([ToTensorV2(p=1.0)],
                         bbox_params=A.BboxParams(format='pascal_voc', min_visibility=0.5, label_fields=['labels']))

def reset_weights(m):
  '''
    Try resetting model weights to avoid
    weight leakage.
  '''
  for layer in m.children():
    if hasattr(layer, 'reset_parameters'):
        print(f'Reset trainable parameters of layer = {layer}')
        layer.reset_parameters()

"""###**4. Visualización de las imágenes**

La siguiente función `plot_img_box` muestra una imagen recibida del conjunto de datos y superpone las cajas delimitadoras provenientes del archivo de anotación.

Posteriormente, se carga un conjunto de datos, selecciona aleatoriamente tres ejemplos, y muestra la imagen junto con las cajas delimitadoras asociadas a las anotaciones para estos ejemplos específicos.

Esto nos resulta útil para verificar visualmente cómo se están procesando y anotando las imágenes en el conjunto de datos.
"""

# Function to visualize bounding boxes in the image
def plot_img_bbox(img, target):
    # plot the image and bboxes
    # Bounding boxes are defined as follows: x-min y-min width height
    fig, a = plt.subplots(1, 1)
    fig.set_size_inches(5, 5)
    a.imshow(img.permute((1,2,0)))
    for box in (target['boxes']):
        x, y, width, height = box[0], box[1], box[2] - box[0], box[3] - box[1]
        rect = patches.Rectangle((x, y),
                                 width, height,
                                 edgecolor='b',
                                 facecolor='none',
                                 clip_on=False)
        a.annotate('Crater', (x,y-20), color='blue', weight='bold',
                   fontsize=10, ha='left', va='top')

        # Draw the bounding box on top of the image
        a.add_patch(rect)
    plt.show()

dataset = CraterDataset('craters/train', get_transform(train=True))
# Prints an example of image with annotations
for i in random.sample(range(1, 100), 3):
    img, target = dataset[i]
    plot_img_bbox(img, target)

"""Hemos cambiado la ruta de los datos para tener las imagenes que queríamos

###**5. K-Fold Cross Validation**

Cross-validation (validación cruzada) es una técnica comúnmente utilizada para evaluar el rendimiento de un modelo de machine learning y mitigar el riesgo de sobreajuste (overfitting). Su objetivo principal es obtener una estimación más robusta del rendimiento del modelo al evaluarlo en diferentes subconjuntos de datos.

El proceso de validación cruzada implica dividir el conjunto de datos en múltiples particiones, denominadas "folds" o "pliegues".

Luego, se entrena y evalúa el modelo en varias iteraciones, utilizando diferentes combinaciones de pliegues como conjunto de entrenamiento y conjunto de prueba en cada iteración.

El tipo más común de validación cruzada es la validación cruzada k-fold, que es la que utilizaremos,  donde el conjunto de datos se divide en k pliegues. El modelo se entrena k veces, cada vez utilizando k-1 pliegues como conjunto de entrenamiento y el pliegue restante como conjunto de prueba. Esto proporciona k estimaciones del rendimiento del modelo, y la métrica de rendimiento final se calcula promediando estas estimaciones.

En esta parte se modificó la función evaluate ya que no funcionaba y se podía prescindir de ella para hacer la validación cruzada y posteriormente entrenar el modelo.
"""

# train on the GPU or on the CPU, if a GPU is not available
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
k_folds = 2
num_epochs = 10


# our dataset has two classes only - background and crater
num_classes = 2
# use our dataset and defined transformations
dataset = CraterDataset('craters/train', get_transform(train=True))
dataset_val = CraterDataset('craters/train', get_transform(train=False))

# Define the K-fold Cross Validator
kfold = KFold(n_splits=k_folds, shuffle=True)

# Start print
print('--------------------------------')

# K-fold Cross Validation model evaluation
for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):
    print(f'FOLD {fold}')
    print('--------------------------------')

    dataset_subset = torch.utils.data.Subset(dataset, list(train_ids))
    dataset_val_subset = torch.utils.data.Subset(dataset_val, list(val_ids))

    # define training and validation data loaders
    data_loader = torch.utils.data.DataLoader(
            dataset_subset, batch_size=8, shuffle=True, num_workers=2,
        collate_fn=utils.collate_fn)

    data_loader_val = torch.utils.data.DataLoader(
        dataset_val_subset, batch_size=1, shuffle=False, num_workers=2,
        collate_fn=utils.collate_fn)

    # get the model using our helper function
    model = get_model_bbox(num_classes)

    #model.apply(reset_weights) # Check if beneficial

    # move model to the right device
    model.to(device)

    # construct an optimizer
    params = [p for p in model.parameters() if p.requires_grad]
    optimizer = torch.optim.SGD(params, lr=0.005,  # Check if beneficial
                                momentum=0.9, weight_decay=0)

    # and a learning rate scheduler
    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,
                                                    step_size=10,
                                                    gamma=0.1)

    # let's train!
    for epoch in range(num_epochs):


        # train for one epoch, printing every 50 iterations
        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=50)
        # update the learning rate
        lr_scheduler.step()
        # evaluate on the test dataset
    # ELIMINAMOS ESTO evaluate(model, data_loader_val, device=device)

"""Part 6: Final training and evaluation
In the final training the selected model configuration was run for 200 epochs on the combined train and validation datasets, saving the best score metric of mAP@IoU:0.5 on each epoch. Due to the small size of 19 images selected for testing, which may well be biased in some manner, the best scoring epoch was achieved in the first few epochs. After which thetrain loss metric continued to improve but the evaluation metrics on the test dataset got worse.

###**6. Entrenamietno final del modelo**

El entrenamiento se modificó y configuró para 100 épocas en los conjuntos de datos de entrenamiento y validación descargados de Google Drive combinados.
"""

num_epochs = 100
#Bajamos el número a 100 para que sea más lijero computacionalmente.

# our dataset has two classes only - background and crater
num_classes = 2
# use our dataset and defined transformations
dataset = CraterDataset('craters/train', get_transform(train=True))
dataset_test = CraterDataset('craters/test', get_transform(train=False))

# define training and validation data loaders
data_loader = torch.utils.data.DataLoader(
        dataset, batch_size=8, shuffle=True, num_workers=2,
    collate_fn=utils.collate_fn)

data_loader_test = torch.utils.data.DataLoader(
    dataset_test, batch_size=1, shuffle=False, num_workers=2,
    collate_fn=utils.collate_fn)

# get the model using our helper function
model = get_model_bbox(num_classes)

'''
Use this to reset all trainable weights
model.apply(reset_weights)
'''

# move model to the right device
model.to(device)

# construct an optimizer
params = [p for p in model.parameters() if p.requires_grad]
optimizer = torch.optim.SGD(params, lr=0.005,  # Feel free to play with values
                            momentum=0.9, weight_decay=0)

# Defining learning rate scheduler
lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,
                                                step_size=20,
                                                gamma=0.2)


result_mAP = []
best_epoch = None

# Let's train!
for epoch in range(num_epochs):


    # train for one epoch, printing every 10 iterations
    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=50)
    # update the learning rate
    lr_scheduler.step()
    # evaluate on the test dataset
    #OTRA VEZ QUITAMOS ESTA PAFRTE PORQUE DA ERROR: results =  evaluate(model, data_loader_test, device=device)
    # saves results of mAP @ IoU = 0.5
    #esult_mAP.append(results.coco_eval['bbox'].stats[1])
    #save the best model so far
    #if result_mAP[-1] == max(result_mAP):
        #best_save_path = os.path.join(f'Crater_bestmodel_noaug_sgd(wd=0)_8batch-epoch{epoch}.pth')
        #torch.save(model.state_dict(), best_save_path)
        #best_epoch = int(epoch)
        #print(f'\n\nmodel from epoch number {epoch} saved!\n result is {max(result_mAP)}\n\n')

# Saving the last model
save_path = os.path.join(f'Crater_noaug_sgd_2batch-lastepoch{num_epochs-1}.pth')
torch.save(model.state_dict(), save_path)
print(f'model from last epoch(no.{num_epochs-1}) saved')

"""###**7.Inferencia**

Finalmente, se carga el modelo con mejor rendimiento en el conjunto de datos de prueba y se realiza la inferencia aquí para mostrar y guardar las imágenes con la etiquetado original y las predicciones del modelo, con un umbral de confianza establecido en 0.7.

Además, se calcula el número de cuadros por segundo (fps) para las predicciones y la escritura de archivos.
"""

dataset_test = CraterDataset('craters/test', get_transform(train=False))

data_loader_test = torch.utils.data.DataLoader(
    dataset_test, batch_size=1, shuffle=False, num_workers=2,
    collate_fn=utils.collate_fn)

num_classes=2
model = get_model_bbox(num_classes)
num_epochs=100

# load model to evaluate
model.eval()
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.load_state_dict(torch.load(f'Crater_noaug_sgd_2batch-lastepoch{num_epochs-1}.pth'))
model.to(device)

# Define colors for bounding boxes
color_inference = np.array([0.0,0.0,255.0])
color_label = np.array([255.0,0.0,0.0])

# Score value thershold for displaying predictions
detection_threshold = 0.7
# to count the total number of images iterated through
frame_count = 0
# to keep adding the FPS for each image
total_fps = 0

!mkdir ./results

for i,data in enumerate(data_loader_test):
    # get the image file name for predictions file name
    image_name = 'image no:' + str(int(data[1][0]['image_id']))
    model_image = data[0][0]
    cv2_image = np.transpose(model_image.numpy()*255,(1, 2, 0)).astype(np.float32)
    cv2_image = cv2.cvtColor(cv2_image, cv2.COLOR_RGB2BGR).astype(np.float32)

    # add batch dimension
    model_image = torch.unsqueeze(model_image, 0)
    start_time = time.time()
    with torch.no_grad():
        outputs = model(model_image.to(device))
    end_time = time.time()
    # get the current fps
    fps = 1 / (end_time - start_time)
    # add `fps` to `total_fps`
    total_fps += fps
    # increment frame count
    frame_count += 1
    # load all detection to CPU for further operations
    outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]
    # carry further only if there's detected boxes
    if len(outputs[0]['boxes']) != 0:
        boxes = outputs[0]['boxes'].data.numpy()
        scores = outputs[0]['scores'].data.numpy()
        # filter out boxes according to `detection_threshold`
        boxes = boxes[scores >= detection_threshold].astype(np.int32)
        scores = np.round(scores[scores >= detection_threshold],2)
        draw_boxes = boxes.copy()


        # draw the bounding boxes and write the class name on top of it
        for j,box in enumerate(draw_boxes):
            cv2.rectangle(cv2_image,
                          (int(box[0]), int(box[1])),
                          (int(box[2]), int(box[3])),
                          color_inference, 2)
            cv2.putText(img=cv2_image, text="Crater",
                        org=(int(box[0]), int(box[1] - 5)),
                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,fontScale= 0.3,color= color_inference,
                         thickness=1, lineType=cv2.LINE_AA)
            cv2.putText(img=cv2_image, text=str(scores[j]),
                        org=(int(box[0]), int(box[1] + 8)),
                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,fontScale= 0.3,color= color_inference,
                        thickness=1, lineType=cv2.LINE_AA)

        # add boxes for labels
        for box in data[1][0]['boxes']:
            cv2.rectangle(cv2_image,
                          (int(box[0]), int(box[1])),
                          (int(box[2]), int(box[3])),
                          color_label, 2)
            cv2.putText(img=cv2_image, text="Label",
                        org=(int(box[0]), int(box[1] - 5)),
                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,fontScale= 0.3,color= color_label,
                        thickness=1, lineType=cv2.LINE_AA)


        # set size
        plt.figure(figsize=(10,10))
        plt.axis("off")

        # convert color from CV2 BGR back to RGB
        plt_image = cv2.cvtColor(cv2_image/255.0, cv2.COLOR_BGR2RGB)
        plt.imshow(plt_image)
        plt.show()
        cv2.imwrite(f"./results/{image_name}.jpg", cv2_image)
    print(f"Image {i + 1} done...")
    print('-' * 50)
print('TEST PREDICTIONS COMPLETE')

avg_fps = total_fps / frame_count
print(f"Average FPS: {avg_fps:.3f}")

"""#**RetinaNet R-CNN**

RetinaNet es un modelo de detección de objetos diseñado para abordar el desafío de la detección precisa y eficiente de objetos en imágenes. Una de las características distintivas de RetinaNet es su capacidad para manejar objetos a diferentes escalas, incluso objetos pequeños y grandes, mediante el uso de una arquitectura de red especializada.

RetinaNet utiliza una arquitectura FPN como su columna vertebral (backbone). FPN ayuda a manejar objetos a diferentes escalas/tamaños de manera más efectiva al construir una pirámide de características con niveles de resolución progresivamente más bajos.

Una contribución clave de RetinaNet es la introducción de la función de pérdida focal. Esta función de pérdida prioriza el manejo de objetos difíciles, como aquellos que son pequeños o tienen una apariencia menos distintiva, mejorando así la capacidad del modelo para aprender de manera efectiva en presencia de desequilibrios de clases.

###**1. Introducción y configuración del entorno**

Como en el modelo anterior, instalamos las dependencias y bibliotecas necesarias, así como los mismos datos de entrenamiento.

Se sustituye el modelo importado de TorchVision por el modelo de RetinaNet.

(En este notebook podríamos omitir ciertas partes de este paso pero asumimos que empezamos de cero)
"""

# Download TorchVision repo to use some files from
# references/detection
!git clone https://github.com/pytorch/vision.git
!cd vision
!git checkout v0.8.2

!cp ./vision/references/detection/utils.py ./
!cp ./vision/references/detection/transforms.py ./
!cp ./vision/references/detection/coco_eval.py ./
!cp ./vision/references/detection/engine.py ./
!cp ./vision/references/detection/coco_utils.py ./

!pip install cython
# Install pycocotools, the version by default in Colab
!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'
!pip install -U albumentations
!pip install -U opencv-python

!pip install gdown #Descargamos nuestro conjunto de datos de Google Drive
!gdown https://drive.google.com/uc?id=1hrHgANwgC8VyXLhXgxLYHTrILpeJ5bLl
!unzip /content/mars_and_moon.zip

import os #Biblioteca con funciones para interactuar con el sistema operativo, como trabajar con rutas de archivos y directorios.
import numpy as np #Biblioteca para cálculos numéricos: análisis de datos y manipulación de matrices.
import torch #PyTorch: construir y entrenar modelos de aprendizaje profundo.
import torchvision #Biblioteca que forma parte de PyTorch y proporciona modelos preentrenados y utilidades para la visión por computadora.
from torchvision.models.detection import RetinaNet
from torchvision.models.detection.retinanet import retinanet_resnet50_fpn #Módulo específico de TorchVision que se utiliza para definir el predictor de la capa de detección de objetos Faster R-CNN.
from engine import train_one_epoch, evaluate
import utils
import transforms as T #Transformaciones de datos utilizadas durante el entrenamiento y la evaluación.
import albumentations as A #Biblioteca que se utiliza para aplicar transformaciones de datos, como aumentaciones, a imágenes y etiquetas de detección de objetos.
import cv2 #OpenCV: biblioteca de visión por computadora que se utiliza para trabajar con imágenes y aplicar diversas operaciones de procesamiento de imágenes.
import time #Esta biblioteca proporciona funciones relacionadas con la medición del tiempo y se utiliza para calcular el tiempo que lleva ejecutar ciertas operaciones.
from albumentations.pytorch.transforms import ToTensorV2 #Importa una transformación de Albumentations que convierte las imágenes en tensores de PyTorch.
import matplotlib #Biblioteca para crear gráficos y visualizaciones
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from  sklearn.model_selection import KFold
import random #Esta biblioteca proporciona funciones relacionadas con la generación de números aleatorios y se utiliza para realizar selecciones aleatorias, como la selección de muestras de datos durante el entrenamiento cruzado (K-fold).

"""###**2. Preparación de los datos. Setup de las clase Dataset**

Se crea la misma clase CraterDataser para el manejo de los datos en la red neuronal pues al utilizar una ResNet50 como columna vertebral (backbone) tanto en RetinaNet como en Faster R-CNN, las características de los datos de entrada son esencialmente las mismas en ambas arquitecturas.

La columna vertebral ResNet50 se encarga de extraer características de las imágenes de entrada y proporciona representaciones jerárquicas que son utilizadas por las cabezas de detección respectivas.
"""

class CraterDataset(object):
    def __init__(self, root, transforms):
        self.root = root
        self.transforms = transforms
        # load all image files, sorting them to
        # ensure that they are aligned
        self.imgs = list(sorted(os.listdir(os.path.join(self.root, "images"))))
        self.annots = list(sorted(os.listdir(os.path.join(self.root, "labels"))))
        self.classes = ['Background','Crater']

    # Converts boundry box formats, this version assumes single class only!
    def convert_box_cord(self,bboxs, format_from, format_to, img_shape):
        if format_from == 'normxywh':
            if format_to == 'xyminmax':
                xw = bboxs[:, (1, 3)] * img_shape[1]
                yh = bboxs[:, (2, 4)] * img_shape[0]
                xmin = xw[:, 0] - xw[:, 1] / 2
                xmax = xw[:, 0] + xw[:, 1] / 2
                ymin = yh[:, 0] - yh[:, 1] / 2
                ymax = yh[:, 0] + yh[:, 1] / 2
                coords_converted = np.column_stack((xmin, ymin, xmax, ymax))

        return coords_converted

    def __getitem__(self, idx):
        # load images and boxes
        img_path = os.path.join(self.root, "images", self.imgs[idx])
        annot_path = os.path.join(self.root, "labels", self.annots[idx])
        img = cv2.imread(img_path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)
        img= img/255.0

        # retrieve bbox list and format to required type,
        # if annotation file is empty, fill dummy box with label 0
        if os.path.getsize(annot_path) != 0:
            bboxs = np.loadtxt(annot_path, ndmin=2)
            bboxs = self.convert_box_cord(bboxs, 'normxywh', 'xyminmax', img.shape)
            num_objs = len(bboxs)
            bboxs = torch.as_tensor(bboxs, dtype=torch.float32)
            # there is only one class
            labels = torch.ones((num_objs,), dtype=torch.int64)
            # suppose all instances are not crowd
            iscrowd = torch.zeros((num_objs,), dtype=torch.int64)
        else:
            bboxs = torch.as_tensor([[0, 0, 640, 640]], dtype=torch.float32)
            labels = torch.zeros((1,), dtype=torch.int64)
            iscrowd = torch.zeros((1,), dtype=torch.int64)

        area = (bboxs[:, 3] - bboxs[:, 1]) * (bboxs[:, 2] - bboxs[:, 0])
        image_id = torch.tensor([idx])

        target = {}
        target["boxes"] = bboxs
        target["labels"] = labels
        target["image_id"] = image_id
        target["area"] = area
        target["iscrowd"] = iscrowd

        if self.transforms is not None:
            sample = self.transforms(image=img,
                                     bboxes=target['boxes'],
                                     labels=labels)
        img = sample['image']
        target['boxes'] = torch.tensor(sample['bboxes'])
        target['labels'] = torch.tensor(sample['labels'])
        if target['boxes'].ndim == 1:
            target['boxes'] = torch.as_tensor([[0, 0, 640, 640]], dtype=torch.float32)
            target['labels'] = torch.zeros((1,), dtype=torch.int64)
        return img, target

    def __len__(self):
        return len(self.imgs)

"""###**3. Configuración del modelo**

Definimos el modelo de RetinaNet
"""

def get_model_bbox1(num_classes):
    # load a RetinaNet model pre-trained on COCO
    model1 = torchvision.models.detection.retinanet_resnet50_fpn(num_classes, pretrained=True)
    return model1

def get_transform(train):
    if train:
        return A.Compose([
            # A.Flip(p=0.5),
            # A.RandomResizedCrop(height=640,width=640,p=0.4),
            # # A.Perspective(p=0.4),
            # A.Rotate(p=0.5),
            # # A.Transpose(p=0.3),
            ToTensorV2(p=1.0)],
            bbox_params=A.BboxParams(format='pascal_voc',min_visibility=0.4, label_fields=['labels']))
    else:
        return A.Compose([ToTensorV2(p=1.0)],
                         bbox_params=A.BboxParams(format='pascal_voc', min_visibility=0.5, label_fields=['labels']))

def reset_weights(m):
  '''
    Try resetting model weights to avoid
    weight leakage.
  '''
  for layer in m.children():
    if hasattr(layer, 'reset_parameters'):
        print(f'Reset trainable parameters of layer = {layer}')
        layer.reset_parameters()

"""###**4.Visualización de las imágenes**

Para la visualización de las imágenes mantenemos las mismas funciones, dado que vamos a necesitar los mismos datos con las mismas bounding boxes.
"""

# Function to visualize bounding boxes in the image
def plot_img_bbox(img, target):
    # plot the image and bboxes
    # Bounding boxes are defined as follows: x-min y-min width height
    fig, a = plt.subplots(1, 1)
    fig.set_size_inches(5, 5)
    a.imshow(img.permute((1,2,0)))
    for box in (target['boxes']):
        x, y, width, height = box[0], box[1], box[2] - box[0], box[3] - box[1]
        rect = patches.Rectangle((x, y),
                                 width, height,
                                 edgecolor='b',
                                 facecolor='none',
                                 clip_on=False)
        a.annotate('Crater', (x,y-20), color='blue', weight='bold',
                   fontsize=10, ha='left', va='top')

        # Draw the bounding box on top of the image
        a.add_patch(rect)
    plt.show()

dataset = CraterDataset('craters/train', get_transform(train=True))
# Prints an example of image with annotations
for i in random.sample(range(1, 100), 3):
    img, target = dataset[i]
    plot_img_bbox(img, target)

"""###**5. K-Fold Cross Validation**

Mantenemos los mismos parámetros para la validación cruzada: número de épocas y de "pliegues"...

El único parámetro cambiado fue el learning rate que bajamos de 0.005 a 0.001: `lr=0.001` para que el modelo no diverja.

También se eliminó la función evaluate del código original.
"""

# train on the GPU or on the CPU, if a GPU is not available
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
k_folds = 2
num_epochs = 10


# our dataset has two classes only - background and crater
num_classes = 2
# use our dataset and defined transformations
dataset = CraterDataset('craters/train', get_transform(train=True))
dataset_val = CraterDataset('craters/train', get_transform(train=False))

# Define the K-fold Cross Validator
kfold = KFold(n_splits=k_folds, shuffle=True)

# Start print
print('--------------------------------')

# K-fold Cross Validation model evaluation
for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):
    print(f'FOLD {fold}')
    print('--------------------------------')

    dataset_subset = torch.utils.data.Subset(dataset, list(train_ids))
    dataset_val_subset = torch.utils.data.Subset(dataset_val, list(val_ids))

    # define training and validation data loaders
    data_loader = torch.utils.data.DataLoader(
            dataset_subset, batch_size=8, shuffle=True, num_workers=2,
        collate_fn=utils.collate_fn)

    data_loader_val = torch.utils.data.DataLoader(
        dataset_val_subset, batch_size=1, shuffle=False, num_workers=2,
        collate_fn=utils.collate_fn)

    # get the model using our helper function
    model1 = get_model_bbox1(num_classes)

    #model.apply(reset_weights) # Check if beneficial

    # move model to the right device
    model1.to(device)

    # construct an optimizer
    params = [p for p in model1.parameters() if p.requires_grad]
    optimizer = torch.optim.SGD(params, lr=0.001,  # Cambiamos la learning rate para que el modelo no diverja
                                momentum=0.9, weight_decay=0)

    # and a learning rate scheduler
    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,
                                                    step_size=10,
                                                    gamma=0.1)

    # let's train!
    for epoch in range(num_epochs):


        # train for one epoch, printing every 50 iterations
        train_one_epoch(model1, optimizer, data_loader, device, epoch, print_freq=50)
        # update the learning rate
        lr_scheduler.step()
        # evaluate on the test dataset
    # ELIMINAMOS ESTO evaluate(model, data_loader_val, device=device)

"""###**6. Entrenamiento final del modelo**

Para el entrenamiento configuramos el mismo número de épocas pero volvemos a configurar una learning rate de 0.001: `lr=0.001`

Guardamos el modelo en una nueva ruta en el directorio.
"""

num_epochs = 100
#Bajamos el número a 100 para que sea más lijero computacionalmente.

# our dataset has two classes only - background and crater
num_classes = 2
# use our dataset and defined transformations
dataset = CraterDataset('craters/train', get_transform(train=True))
dataset_test = CraterDataset('craters/test', get_transform(train=False))

# define training and validation data loaders
data_loader = torch.utils.data.DataLoader(
        dataset, batch_size=8, shuffle=True, num_workers=2,
    collate_fn=utils.collate_fn)

data_loader_test = torch.utils.data.DataLoader(
    dataset_test, batch_size=1, shuffle=False, num_workers=2,
    collate_fn=utils.collate_fn)

# get the model using our helper function
model1 = get_model_bbox1(num_classes)

'''
Use this to reset all trainable weights
model.apply(reset_weights)
'''

# move model to the right device
model1.to(device)

# construct an optimizer
params = [p for p in model1.parameters() if p.requires_grad]
optimizer = torch.optim.SGD(params, lr=0.001,  # Feel free to play with values
                            momentum=0.9, weight_decay=0)

# Defining learning rate scheduler
lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,
                                                step_size=20,
                                                gamma=0.2)


result_mAP = []
best_epoch = None

# Let's train!
for epoch in range(num_epochs):


    # train for one epoch, printing every 10 iterations
    train_one_epoch(model1, optimizer, data_loader, device, epoch, print_freq=50)
    # update the learning rate
    lr_scheduler.step()
    # evaluate on the test dataset
    #OTRA VEZ QUITAMOS ESTA PAFRTE PORQUE DA ERROR: results =  evaluate(model, data_loader_test, device=device)
    # saves results of mAP @ IoU = 0.5
    #esult_mAP.append(results.coco_eval['bbox'].stats[1])
    #save the best model so far
    #if result_mAP[-1] == max(result_mAP):
        #best_save_path = os.path.join(f'Crater_bestmodel_noaug_sgd(wd=0)_8batch-epoch{epoch}.pth')
        #torch.save(model.state_dict(), best_save_path)
        #best_epoch = int(epoch)
        #print(f'\n\nmodel from epoch number {epoch} saved!\n result is {max(result_mAP)}\n\n')

# Saving the last model
save_path1 = os.path.join(f'Crater_noaug_sgd_2batch-lastepoch{num_epochs-1}Retina.pth')
torch.save(model1.state_dict(), save_path1)
print(f'model from last epoch(no.{num_epochs-1}) saved')

"""###**7. Inferencia**

Cargamos el nuevo modelo guardado en la nueva ruta para los datos de test y comprobar el resultado que obtenemos.
"""

dataset_test = CraterDataset('craters/test', get_transform(train=False))

data_loader_test = torch.utils.data.DataLoader(
    dataset_test, batch_size=1, shuffle=False, num_workers=2,
    collate_fn=utils.collate_fn)

num_classes=2
model1 = get_model_bbox1(num_classes)
num_epochs=100

# load model to evaluate
model1.eval()
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model1.load_state_dict(torch.load(f'Crater_noaug_sgd_2batch-lastepoch{num_epochs-1}Retina.pth'))
model1.to(device)

# Define colors for bounding boxes
color_inference = np.array([0.0,255.0,0.0])
color_label1 = np.array([255.0,0.0,0.0])

# Score value thershold for displaying predictions
detection_threshold = 0.7
# to count the total number of images iterated through
frame_count = 0
# to keep adding the FPS for each image
total_fps = 0

!mkdir ./results

for i,data in enumerate(data_loader_test):
    # get the image file name for predictions file name
    image_name = 'image no:' + str(int(data[1][0]['image_id']))
    model_image = data[0][0]
    cv2_image = np.transpose(model_image.numpy()*255,(1, 2, 0)).astype(np.float32)
    cv2_image = cv2.cvtColor(cv2_image, cv2.COLOR_RGB2BGR).astype(np.float32)

    # add batch dimension
    model_image = torch.unsqueeze(model_image, 0)
    start_time = time.time()
    with torch.no_grad():
        outputs = model1(model_image.to(device))
    end_time = time.time()
    # get the current fps
    fps = 1 / (end_time - start_time)
    # add `fps` to `total_fps`
    total_fps += fps
    # increment frame count
    frame_count += 1
    # load all detection to CPU for further operations
    outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]
    # carry further only if there's detected boxes
    if len(outputs[0]['boxes']) != 0:
        boxes = outputs[0]['boxes'].data.numpy()
        scores = outputs[0]['scores'].data.numpy()
        # filter out boxes according to `detection_threshold`
        boxes = boxes[scores >= detection_threshold].astype(np.int32)
        scores = np.round(scores[scores >= detection_threshold],2)
        draw_boxes = boxes.copy()


        # draw the bounding boxes and write the class name on top of it
        for j,box in enumerate(draw_boxes):
            cv2.rectangle(cv2_image,
                          (int(box[0]), int(box[1])),
                          (int(box[2]), int(box[3])),
                          color_inference, 2)
            cv2.putText(img=cv2_image, text="Crater",
                        org=(int(box[0]), int(box[1] - 5)),
                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,fontScale= 0.3,color= color_inference,
                         thickness=1, lineType=cv2.LINE_AA)
            cv2.putText(img=cv2_image, text=str(scores[j]),
                        org=(int(box[0]), int(box[1] + 8)),
                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,fontScale= 0.3,color= color_inference,
                        thickness=1, lineType=cv2.LINE_AA)

        # add boxes for labels
        for box in data[1][0]['boxes']:
            cv2.rectangle(cv2_image,
                          (int(box[0]), int(box[1])),
                          (int(box[2]), int(box[3])),
                          color_label1, 2)
            cv2.putText(img=cv2_image, text="Label",
                        org=(int(box[0]), int(box[1] - 5)),
                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,fontScale= 0.3,color= color_label1,
                        thickness=1, lineType=cv2.LINE_AA)


        # set size
        plt.figure(figsize=(10,10))
        plt.axis("off")

        # convert color from CV2 BGR back to RGB
        plt_image = cv2.cvtColor(cv2_image/255.0, cv2.COLOR_BGR2RGB)
        plt.imshow(plt_image)
        plt.show()
        cv2.imwrite(f"./results/{image_name}.jpg", cv2_image)
    print(f"Image {i + 1} done...")
    print('-' * 50)
print('TEST PREDICTIONS COMPLETE')

avg_fps = total_fps / frame_count
print(f"Average FPS: {avg_fps:.3f}")

"""##**CONCLUSIONES**

En conclusión, podemos observar que el modelo RetinaNet ha realizado una peor detección.

Suponemos que esto ha podido deberse a la implementación inespecífica del modelo RetinaNet: las bibliotecas utilizadas y las configuraciones pueden tener un impacto en el rendimiento. Es posible que una implementación de RetinaNet  quetenga detalles específicos podrían afectar el rendimiento positivamente.

También puede deberse a que cada modelo de detección puede ser sensible a la configuración de los parámetros durante el entrenamiento. Ajustarlos correctamente, como por ejemplo la tasa de aprendizaje puede ser beneficioso para el rendimiento.

Además, la selección del umbral de confianza para las predicciones podrían también haber afectado el rendimiento del modelo. Un umbral demasiado alto puede generar menos detecciones, mientras que un umbral demasiado bajo puede aumentar el número de falsos positivos.

Finalmente, si trabajásemos un poco más en el modelo podríamos llevar a cabo una optimización más exhaustiva para lograr el mejor rendimiento de RetinaNet. Esto implica experimentar con diferentes configuraciones y estrategias.

##**ANEXO MASK RCNN**

Por último, incluimos un intento que realizamos con el modelo Mask RCNN que finalmente no salió.

Tuvimos un error de tipos de datos con las máscaras que no supimos identificar y con los que no fuimos capaces de seguir con el modelo.
"""

# Download TorchVision repo to use some files from
# references/detection
!git clone https://github.com/pytorch/vision.git
!cd vision
!git checkout v0.8.2

!cp ./vision/references/detection/utils.py ./
!cp ./vision/references/detection/transforms.py ./
!cp ./vision/references/detection/coco_eval.py ./
!cp ./vision/references/detection/engine.py ./
!cp ./vision/references/detection/coco_utils.py ./

!pip install cython
# Install pycocotools, the version by default in Colab
!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'
!pip install -U albumentations
!pip install -U opencv-python

!pip install gdown #Descargamos nuestro conjunto de datos de Google Drive
!gdown https://drive.google.com/uc?id=1hrHgANwgC8VyXLhXgxLYHTrILpeJ5bLl
!unzip /content/mars_and_moon.zip

import os #Biblioteca con funciones para interactuar con el sistema operativo, como trabajar con rutas de archivos y directorios.
import numpy as np #Biblioteca para cálculos numéricos: análisis de datos y manipulación de matrices.
import torch #PyTorch: construir y entrenar modelos de aprendizaje profundo.
import torchvision #Biblioteca que forma parte de PyTorch y proporciona modelos preentrenados y utilidades para la visión por computadora.
from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor #Módulo específico de TorchVision que se utiliza para definir el predictor de la capa de detección de objetos Faster R-CNN.
from engine import train_one_epoch, evaluate
import utils
import transforms as T #Transformaciones de datos utilizadas durante el entrenamiento y la evaluación.
import albumentations as A #Biblioteca que se utiliza para aplicar transformaciones de datos, como aumentaciones, a imágenes y etiquetas de detección de objetos.
import cv2 #OpenCV: biblioteca de visión por computadora que se utiliza para trabajar con imágenes y aplicar diversas operaciones de procesamiento de imágenes.
import time #Esta biblioteca proporciona funciones relacionadas con la medición del tiempo y se utiliza para calcular el tiempo que lleva ejecutar ciertas operaciones.
from albumentations.pytorch.transforms import ToTensorV2 #Importa una transformación de Albumentations que convierte las imágenes en tensores de PyTorch.
import matplotlib #Biblioteca para crear gráficos y visualizaciones
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from  sklearn.model_selection import KFold
import random #Esta biblioteca proporciona funciones relacionadas con la generación de números aleatorios y se utiliza para realizar selecciones aleatorias, como la selección de muestras de datos durante el entrenamiento cruzado (K-fold).

class CraterDataset(object):
    def __init__(self, root, transforms):
        self.root = root
        self.transforms = transforms
        # load all image files, sorting them to ensure that they are aligned
        self.imgs = list(sorted(os.listdir(os.path.join(self.root, "images"))))
        self.annots = list(sorted(os.listdir(os.path.join(self.root, "labels"))))
        self.classes = ['Background','Crater']

    # Converts boundry box formats, this version assumes single class only!
    def convert_box_cord(self,bboxs, format_from, format_to, img_shape):
        if format_from == 'normxywh':
            if format_to == 'xyminmax':
                xw = bboxs[:, (1, 3)] * img_shape[1]
                yh = bboxs[:, (2, 4)] * img_shape[0]
                xmin = xw[:, 0] - xw[:, 1] / 2
                xmax = xw[:, 0] + xw[:, 1] / 2
                ymin = yh[:, 0] - yh[:, 1] / 2
                ymax = yh[:, 0] + yh[:, 1] / 2
                coords_converted = np.column_stack((xmin, ymin, xmax, ymax))

        return coords_converted

    def __getitem__(self, idx):
      # load images, boxes, and masks
      img_path = os.path.join(self.root, "images", self.imgs[idx])
      annot_path = os.path.join(self.root, "labels", self.annots[idx])
      img = cv2.imread(img_path)
      img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)
      img= img/255.0

      # retrieve bbox list and format to required type
      # if annotation file is empty, fill dummy box with label 0
      if os.path.getsize(annot_path) !=0:
        bboxs = np.loadtxt(annot_path, ndmin=2)
        bboxs = self.convert_box_cord(bboxs, 'normxywh', 'xyminmax', img.shape)
        num_objs = len(bboxs)
        bboxs = torch.as_tensor(bboxs, dtype=torch.float32)
        # there is only one class
        labels = torch.ones((num_objs,), dtype=torch.int64)
        # suppose all instances are not crowd
        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)
      else:
        bboxs = torch.as_tensor([[0, 0, 640, 640]], dtype=torch.float32)
        labels = torch.zeros((1,), dtype=torch.int64)
        iscrowd = torch.zeros((1,), dtype=torch.int64)

      # Generamos las máscaras para cada bounding box
      masks = []
      for bbox in bboxs:
        mask = np.zeros(img.shape[:2], dtype=np.uint8)
        x1, y1, x2, y2 = bbox
        mask[y1:y2, x1:x2] = 255
        mask = cv2.GaussianBlur(mask, (5, 5), 0)
        _, mask = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
        masks.append(mask)

      masks = np.stack(masks, axis=0)
      masks = (masks > 0).astype(np.uint8)  # Ensure binary masks
      masks = torch.as_tensor(masks, dtype=torch.uint8)

      area = (bboxs[:, 3] - bboxs[:, 1]) * (bboxs[:, 2] - bboxs[:, 0])
      image_id = torch.tensor([idx])

      target = {}
      target["boxes"] = bboxs
      target["labels"] = labels
      target["masks"] = masks
      target["image_id"] = image_id
      target["area"] = area
      target["iscrowd"] = iscrowd

      if self.transforms is not None:
        sample = self.transforms(image=img,
                              bboxes=target['boxes'],
                              labels=labels,
                              masks=target['masks'])
      img = sample['image']
      target['boxes'] = torch.tensor(sample['bboxes'])
      target['labels'] = torch.tensor(sample['labels'])
      target['masks'] = torch.as_tensor(sample['masks'])

      if target['boxes'].ndim == 1:
            target['boxes'] = torch.as_tensor([[0, 0, 640, 640]], dtype=torch.float32)
            target['labels'] = torch.zeros((1,), dtype=torch.int64)
      return img, target

    def __len__(self):
      return len(self.imgs)

"""Después de este punto, tras un intento de crear las máscaras a partir de las bounding boxes de cada imágen, recibimos el error en la validación cruzada de K-Fold."""

def get_model_bbox(num_classes):
    # load an instance segmentation model pre-trained on COCO
    model = torchvision.models.detection.mask_rcnn.MaskRCNN(pretrained=True)

    # get number of input features for the classifier
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    # replace the pre-trained head with a new one
    model.roi_heads.box_predictor = MaskRCNNPredictor(in_features, num_classes)

    return model

import torchvision
from torchvision.models.detection import FasterRCNN
from torchvision.models.detection.rpn import AnchorGenerator
from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor

def get_model_mask(num_classes):
    # load a pre-trained Faster R-CNN model
    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)

    # Numero de inputs para máscaras
    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels
    hidden_layer = 256  # define the number of hidden units for the CRNN

    # Sustituir la cabeza de mask predictor por otra.
    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)

    return model

def get_transform(train):
    if train:
        return A.Compose([
            # A.Flip(p=0.5),
            # A.RandomResizedCrop(height=640,width=640,p=0.4),
            # # A.Perspective(p=0.4),
            # A.Rotate(p=0.5),
            # # A.Transpose(p=0.3),
            ToTensorV2(p=1.0)],
            bbox_params=A.BboxParams(format='pascal_voc',min_visibility=0.4, label_fields=['labels']))
    else:
        return A.Compose([ToTensorV2(p=1.0)],
                         bbox_params=A.BboxParams(format='pascal_voc', min_visibility=0.5, label_fields=['labels']))

def reset_weights(m):
  '''
    Try resetting model weights to avoid
    weight leakage.
  '''
  for layer in m.children():
    if hasattr(layer, 'reset_parameters'):
        print(f'Reset trainable parameters of layer = {layer}')
        layer.reset_parameters()

import matplotlib.pyplot as plt
import matplotlib.patches as patches

def plot_img_bbox(img, target):
    # plot the image and bboxes
    # Bounding boxes are defined as follows: x-min y-min width height
    fig, a = plt.subplots(1, 1)
    fig.set_size_inches(5, 5)
    a.imshow(img.permute((1, 2, 0)).numpy())

    # Iterar sobre mascaras y boxes
    for box, mask in zip(target['boxes'], target['masks']):
        x, y, width, height = box[0], box[1], box[2] - box[0], box[3] - box[1]
        rect = patches.Rectangle((x, y),
                                 width, height,
                                 edgecolor='b',
                                 facecolor='none',
                                 clip_on=False)
        a.add_patch(rect)
        # Pintar la máscara
        mask = mask[0].permute(1, 2, 0).numpy()
        a.imshow(mask, alpha=0.7)

    plt.show()

dataset = CraterDataset('craters/train', get_transform(train=True))
# Prints an example of image with annotations
for i in random.sample(range(1, 100), 3):
    img, target = dataset[i]
    plot_img_bbox(img, target)